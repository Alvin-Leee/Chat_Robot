{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则表达式进行文本搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very|nice|lecture|day|moon\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['very', 'nice', 'lecture']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text search across the sentence using Regular expression\n",
    "import re\n",
    "words = ['very', 'nice', 'lecture', 'day', 'moon']\n",
    "expression = '|'.join(words)\n",
    "print(expression)\n",
    "re.findall(expression, 'i attended a very nice lecture last year', re.M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将文本转换为列表\n",
    "### 读取一个文本文件并根据需要将它转化为一列单词或一列句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once', 'when', 'I', 'was', 'six', 'years', 'old', 'I', 'saw', 'a', 'magnificent', 'picture', 'in', 'a', 'book,', 'called', 'True', 'Stories', 'from', 'Nature,', 'about', 'the', 'primeval', 'forest.', 'It', 'was', 'a', 'picture', 'of', 'a', 'boa', 'constrictor', 'in', 'the', 'act', 'of', 'swallowing', 'an', 'animal.', 'Here', 'is', 'a', 'copy', 'of', 'the', 'drawing.', 'In', 'the', 'book', 'it', 'said:', '“Boa', 'constrictors', 'swallow', 'their', 'prey', 'whole,', 'without', 'chewing', 'it.', 'After', 'that', 'they', 'are', 'not', 'able', 'to', 'move,', 'and', 'they', 'sleep', 'through', 'the', 'six', 'months', 'that', 'they', 'need', 'for', 'digestion.”']\n"
     ]
    }
   ],
   "source": [
    "text_file = 'nlp_starts.txt'\n",
    "# Method-1: Individual words as separate elements of the list\n",
    "with open(text_file) as f:\n",
    "    words = f.read().split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once when I was six years old I saw a magnificent picture in a book, called \\n', 'True Stories from Nature, about the primeval forest. It was a picture of a boa \\n', 'constrictor in the act of swallowing an animal. Here is a copy of the drawing. \\n', 'In the book it said: “Boa constrictors swallow their prey whole, without \\n', 'chewing it. After that they are not able to move, and they sleep through the \\n', 'six months that they need for digestion.”\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Method-2: Whole text as single element of the list\n",
    "f = open(text_file, 'r')\n",
    "words_ = f.readlines()\n",
    "print(words_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本预处理\n",
    "#### 比如：将一个单词替换为另一个单词，删除或添加某些特定类型的单词等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'john has been selected for the trial phase this time. congrats'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'John has been selected for the trial phase this time. Congrats!'\n",
    "sentence = sentence.lower()\n",
    "# defining the positive and negative words explicitly\n",
    "positive_words = ['awesome', 'good', 'nice', 'super', 'fun', 'delightful', 'congrats']\n",
    "negative_words = ['awful', 'lame', 'horrible', 'bad']\n",
    "sentence = sentence.replace('!', '')\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'has', 'been', 'selected', 'for', 'the', 'trial', 'phase', 'this', 'time.', 'congrats']\n",
      "{'time.', 'john', 'for', 'trial', 'has', 'been', 'this', 'the', 'phase', 'selected'}\n"
     ]
    }
   ],
   "source": [
    "words = sentence.split(' ')\n",
    "print(words)\n",
    "result = set(words)-set(positive_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从网页中获取文本(urllib函数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure both the packages are installed\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "pool_object = urllib3.PoolManager()\n",
    "target_url = 'www.baidu.com'\n",
    "response_ = pool_object.request('GET', target_url)\n",
    "final_html_txt = BeautifulSoup(response_.data)\n",
    "print(final_html_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 移除停止词\n",
    "### 停止词是搜索引擎会忽略的常用词(例如\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'about', 'Deep', 'Learning', 'and', 'Natural', 'Language', 'Processing', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "sentence = \"This book is about Deep Learning and Natural Language Processing!\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'book', 'Deep', 'Learning', 'Natural', 'Language', 'Processing', '!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_tokens = [w for w in tokens if not w in stop_words]\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计数向量化\n",
    "### 计数向量化是一个SciKit-Learn库工具，它可以接受任何大量的文本，将每个独特的单词作为特征返回，并计算每个单词在文本中出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also', 'and', 'classic', 'classical', 'he', 'listens', 'music', 'old', 'pop', 'ramiess', 'rock', 'sings', 'songs', 'to']\n",
      "[[0 0 1 0 0 0 0 0 0 1 0 1 1 0]\n",
      " [0 0 0 0 1 1 0 1 1 0 0 0 0 1]\n",
      " [0 1 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [1 1 0 1 0 1 0 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts = [\"Ramiess sings classic songs\", \"he listens to old pop \", \"and rock music\", 'and also listens to classical songs']\n",
    "cv = CountVectorizer()\n",
    "cv_fit = cv.fit_transform(texts)\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF分数(Sklearn Package)\n",
    "### TF(term frequency)：术语频率, 表示特定单词的计数与文档中单词总数的比率\n",
    "### IDF(inverse document frequency) ：反向文档频率, 指文档总数与包含特定单词的文档数量的对数比率\n",
    "#### 例：文档中包含100个单词，其中“happy”出现了5次，则TF = （5/100）=0.05\n",
    "#### 假设有1000万个文档，“happy”出现在其中1000个文件中，IDF = log(10000000/1000) = 4\n",
    "#### 则 TF-IDF = 4*0.05 = 0.2\n",
    "### 类似的指标是BM25，它根据文档与查询语句的关系来对文档进行评分。BM25使用每个文档的查询项对一组文档进行排名，而不考虑文档中查询关键字之间的关系如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.52547275 0.         0.         0.\n",
      "  0.         0.         0.         0.52547275 0.         0.52547275\n",
      "  0.41428875 0.        ]\n",
      " [0.         0.         0.         0.         0.48546061 0.38274272\n",
      "  0.         0.48546061 0.48546061 0.         0.         0.\n",
      "  0.         0.38274272]\n",
      " [0.         0.48693426 0.         0.         0.         0.\n",
      "  0.61761437 0.         0.         0.         0.61761437 0.\n",
      "  0.         0.        ]\n",
      " [0.47212003 0.37222485 0.         0.47212003 0.         0.37222485\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37222485 0.37222485]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "texts = [\"Ramiess sings classic songs\", \"he listens to old pop\", \"and rock music\", ' and also listens to classical songs']\n",
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(texts)\n",
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本分类器(TextBlob Package)\n",
    "### 文本可以被分为很多中，例如正面和负面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "data = [\n",
    "    ('I love my country.', 'pos'),\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "    ('I do not like the smell of this place.', 'neg'),\n",
    "    ('I do not like this restaurant', 'neg'),\n",
    "    ('I am tired of hearing your nonsense.', 'neg'),\n",
    "    (\"I always aspire to be like him\", 'pos'),\n",
    "    (\"It's a horrible performance.\", 'neg')\n",
    "]\n",
    "model = NaiveBayesClassifier(data)\n",
    "model.classify(\"It's an awesome place!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
