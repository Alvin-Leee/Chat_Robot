{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec_skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing the required packages'''\n",
    "import random\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "'''Make sure the dataset link is copied correctly'''\n",
    "dataset_link = 'http://mattmahoney.net/dc/'\n",
    "zip_file = 'text8.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载有Matt Mahoney 收集和清理的危机百科文章数据集，并将其存储为当前工作目录下的单独文件\n",
    "def data_download(zip_file):\n",
    "    \"\"\"Download the required file\"\"\"\n",
    "    if not os.path.exists(zip_file):\n",
    "        zip_file, _ = urlretrieve(dataset_link + zip_file, zip_file)\n",
    "        print('File downloaded successfully!')\n",
    "    return None\n",
    "data_download(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩的文本数据集在内部文件夹数据集中提取，稍后将用于训练模型\n",
    "\"\"\"Extracting the dataset in separate folder\"\"\"\n",
    "extracted_folder = 'dataset'\n",
    "\n",
    "if not os.path.isdir(extracted_folder):\n",
    "    with zipfile.ZipFile(zip_file) as zf:\n",
    "        zf.extractall(extracted_folder)\n",
    "        \n",
    "with open('dataset/text8') as ft_:\n",
    "    full_text = ft_.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于输入数据的文本中有多个标点符号和其他符号，相同的符号将被替换为带有标点符号名称和符号类型的相应字符\n",
    "# 有助于让模型单独识别每个标点符号和其他符号并生成向量\n",
    "def text_processing(ft8_text):\n",
    "    \"\"\"Replacing punctuation marks with tokens\"\"\"\n",
    "    ft8_text = ft8_text.lower()\n",
    "    ft8_text = ft8_text.replace('.', '<period>')\n",
    "    ft8_text = ft8_text.replace(',', '<comma>')\n",
    "    ft8_text = ft8_text.replace('\"', '<quotation>')\n",
    "    ft8_text = ft8_text.replace(';', '<semicolon>')\n",
    "    ft8_text = ft8_text.replace('!', '<exclamation>')\n",
    "    ft8_text = ft8_text.replace('?', '<question>')\n",
    "    ft8_text = ft8_text.replace('(', '<paren_l>')\n",
    "    ft8_text = ft8_text.replace(')', '<paren_r>')\n",
    "    ft8_text = ft8_text.replace('--', '<hyphen>')\n",
    "    ft8_text = ft8_text.replace(':', '<colon>')\n",
    "    ft8_text_tokens = ft8_text.split()\n",
    "    return ft8_text_tokens\n",
    "\n",
    "ft_tokens = text_processing(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including']\n"
     ]
    }
   ],
   "source": [
    "# 为了提高所产生的向量表示的质量，建议去除与单词相关的噪音，即输入数据集中词频小于7的单词，因为这些单词没有足够的信息来提供它们的上下文\n",
    "# 可以通过检查单词数和数据集中的分布来调整此阈值，在此处设为7\n",
    "\"\"\"Shortlisting words with frequency more than 7\"\"\"\n",
    "word_cnt = collections.Counter(ft_tokens)\n",
    "shortlisted_words = [w for w in ft_tokens if word_cnt[w] > 7]\n",
    "\n",
    "# 列出数据集中词频最高的几个单词\n",
    "print(shortlisted_words[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of shortlisted words :  16616688\n",
      "Unique number of shortlisted_words:  53721\n"
     ]
    }
   ],
   "source": [
    "# 检查数据集中所有单词的统计信息\n",
    "print(\"Total number of shortlisted words : \", len(shortlisted_words))\n",
    "print(\"Unique number of shortlisted_words: \", len(set(shortlisted_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了处理语料库中存在的独特单词，我们制作了一组单词和它们在训练数据集中的词频\n",
    "# 创建一个字典并将单词转换为整数，反之，将整数转换为单词\n",
    "# 词频最高的单词被赋予最小值0， 其他单词也通过相似方式被赋予数值，从单词转换而来的整数倍存储在一个单独的数组中\n",
    "def dict_creation(shortlisted_words):\n",
    "    \"\"\"The function creates a dictionary of the words present in dataset along with their frequency order\"\"\"\n",
    "    counts = collections.Counter(shortlisted_words)\n",
    "    vocabulary = sorted(counts, key=counts.get, reverse=True)\n",
    "    rev_dictionary_ = {ii: word for ii, word in enumerate(vocabulary)}\n",
    "    # print(rev_dictionary_)\n",
    "    dictionary_ = {word: ii for ii, word in rev_dictionary_.items()}\n",
    "    # print(dictionary_)\n",
    "    return dictionary_, rev_dictionary_\n",
    "\n",
    "dictionary_, rev_dictionary_ = dict_creation(shortlisted_words)\n",
    "words_cnt = [dictionary_[word] for word in shortlisted_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram 模型采用子采样的方法来处理文本中的停止词\n",
    "### 通过在词频上设置阈值，可以消除所有那些词频较高且中心词周围没有任何重要上下文的单词，这带来了更快的训练速度和更好的词向量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram论文中给出的概率分数函数，对于训练集中的每个单词，我们将根据以下公式给定的概率来决定是否将其移除\n",
    "$$ P(w_{i}) = 1-\\left( \\sqrt\\frac{t}{f(w_{i})}\\right)$$\n",
    "### 其中, t是阈值参数，$f(w_{i})$是单词$w_i$在总数据集中的词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creaing the threshold and performing the subsampling\"\"\"\n",
    "thresh = 0.00005\n",
    "word_counts = collections.Counter(words_cnt)\n",
    "total_count = len(words_cnt)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1-np.sqrt(thresh/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in words_cnt if p_drop[word] < random.random()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 当skip-gram模型接受中心词并预测其周围单词时，skipG_target_set_generation()函数以所需格式创建skip-gram模型的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipG_target_set_generation(batch_, batch_index, word_window):\n",
    "    \"\"\"The function combines the words of given word_window size next to the index,\n",
    "    for the SkipGram model\"\"\"\n",
    "    random_num = np.random.randint(1, word_window+1)\n",
    "    words_start = batch_index - random_num if (batch_index - random_num) > 0 else 0\n",
    "    words_stop = batch_index + random_num\n",
    "    window_target = set(batch_[words_start:batch_index] + batch_[batch_index+1:words_stop+1])\n",
    "    return list(window_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skipG_batch_creation()函数调用skipG_target_set_generation()函数，并创建中心词及其周围单词的组合格式，将其作为目标文本并返回批输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipG_batch_creation(short_words, batch_length, word_window):\n",
    "    \"\"\"The function internally makes use of the skipG_target_set_generation() function and\n",
    "    combines each of the label words in the shortlisted_words with the words of word_window size around\"\"\"\n",
    "    batch_cnt = len(short_words)//batch_length\n",
    "    short_words = short_words[:batch_cnt*batch_length]\n",
    "    \n",
    "    for word_index in range(0, len(short_words), batch_length):\n",
    "        input_words, label_words = [], []\n",
    "        word_batch = short_words[word_index:word_index+batch_length]\n",
    "\n",
    "    for index_ in range(len(word_batch)):\n",
    "        batch_input = word_batch[index_]\n",
    "        batch_label = skipG_target_set_generation(word_batch, index_, word_window)\n",
    "        # Appending the label and inputs to the initial list. Replicating input to the size of labels in he window\n",
    "        label_words.extend(batch_label)\n",
    "        input_words.extend([batch_input]*len(batch_label))\n",
    "        yield input_words, label_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注册一个用于skip-gram实现的TensorFlow图，并声明变量的输入和标签占位符，他们将用于按照中心词和周围单词的组合为输入单词和大小不同的批量分配单热编码常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_graph = tf.Graph()\n",
    "with tf_graph.as_default():\n",
    "    input_ = tf.placeholder(tf.int32, [None], name='input_')\n",
    "    label_ = tf.placeholder(tf.int32, [None, None], name='label_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面的代码声明嵌入矩阵的变量，该矩阵的维度等于词汇表的大小和词嵌入向量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf_graph.as_default():\n",
    "    word_embed = tf.Variable(tf.random_uniform((len(rev_dictionary_), 300), -1, -1))\n",
    "    embedding = tf.nn.embedding_lookup(word_embed, input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.train.AdamOprimier使用Adam算法来控制学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The code includes the following :\n",
    "    # Initializing weights and bias to be used in the softmax layer\n",
    "    # Loss function calculation using the Negative Sampling \n",
    "    # Usage of Adam Optimizer\n",
    "    # Negative sampling on 100 words, to be included in the loss function\n",
    "    # 300 is the word embedding vector size\"\"\"\n",
    "vocabulary_size = len(rev_dictionary_)\n",
    "\n",
    "with tf_graph.as_default():\n",
    "    sf_weights = tf.Variable(tf.truncated_normal((vocabulary_size, 300), stddev=0.1))\n",
    "    sf_bias = tf.Variable(tf.zeros(vocabulary_size))\n",
    "    \n",
    "    loss_fn = tf.nn.sampled_softmax_loss(weights=sf_weights, biases=sf_bias, labels=label_,\n",
    "                                         inputs=embedding, num_sampled=100, num_classes=vocabulary_size)\n",
    "    cost_fn = tf.reduce_mean(loss_fn)\n",
    "    optim = tf.train.AdamOptimizer().minimize(cost_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为了确保单词的向量表示保持了单词之间的语义相似性，我们在下面的代码部分生成一个验证集。\n",
    "### 它将在语料库中选择常见和不常见词的组合，并基于词向量之间的余弦相似性返回最接近它们的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The below code performs the following operations : \n",
    "    # Performing validation here by making use of a random selection of 16 words from the dictionary of desired size\n",
    "    # Selecting 8 words randomly from range of 1000\n",
    "    # Using the cosine distance to calculate the similarity between the words\"\"\"\n",
    "with tf_graph.as_default():\n",
    "    validation_cnt = 16\n",
    "    validation_dict = 100\n",
    "    \n",
    "    validation_words = np.array(random.sample(range(validation_dict), validation_cnt//2))\n",
    "    validation_words = np.append(validation_words, random.sample(range(1000, 1000+validation_dict), validation_cnt//2))\n",
    "    validation_data = tf.constant(validation_words, dtype=tf.int32)\n",
    "    \n",
    "    normalization_embed = word_embed / (tf.sqrt(tf.reduce_sum(tf.square(word_embed), 1, keepdims=True)))\n",
    "    validation_embed = tf.nn.embedding_lookup(normalization_embed, validation_data)\n",
    "    word_similarity = tf.matmul(validation_embed, tf.transpose(normalization_embed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在当前工作目录中创建文件夹model_checkpoint以存储模型检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.568842887878418\n",
      "13.640870094299316\n",
      "21.048076629638672\n",
      "27.72291851043701\n",
      "34.18672800064087\n",
      "40.04692459106445\n",
      "47.378024101257324\n",
      "53.42362117767334\n",
      "59.93447208404541\n",
      "65.40264081954956\n",
      "70.12728977203369\n",
      "75.31607007980347\n",
      "80.2936224937439\n",
      "84.5669298171997\n",
      "88.76335191726685\n",
      "92.29589533805847\n",
      "95.77223372459412\n",
      "99.04659080505371\n",
      "102.69746041297913\n",
      "106.1497175693512\n",
      "109.70588755607605\n",
      "112.56033873558044\n",
      "115.28139472007751\n",
      "118.2389907836914\n",
      "121.42175269126892\n",
      "124.29704594612122\n",
      "127.2165846824646\n",
      "130.08540105819702\n",
      "132.50801157951355\n",
      "135.02510333061218\n",
      "137.24190139770508\n",
      "139.11446678638458\n",
      "141.51191818714142\n",
      "143.93525302410126\n",
      "145.9112514257431\n",
      "147.97297751903534\n",
      "149.7645285129547\n",
      "151.6667252779007\n",
      "153.5666607618332\n",
      "155.40721476078033\n",
      "157.13450229167938\n",
      "158.8536570072174\n",
      "160.54844629764557\n",
      "162.00453579425812\n",
      "164.53635728359222\n",
      "166.13683414459229\n",
      "167.88430547714233\n",
      "169.3502563238144\n",
      "171.09871542453766\n",
      "172.48077714443207\n",
      "174.19820153713226\n",
      "175.75819671154022\n",
      "177.21798646450043\n",
      "178.47900366783142\n",
      "180.00726544857025\n",
      "181.07125163078308\n",
      "182.75175559520721\n",
      "184.63931274414062\n",
      "187.3794937133789\n",
      "190.13146662712097\n",
      "191.3048609495163\n",
      "192.16782665252686\n",
      "193.41388070583344\n",
      "194.25868850946426\n",
      "195.7925096154213\n",
      "197.01126033067703\n",
      "197.8322592973709\n",
      "198.48570001125336\n",
      "199.0079505443573\n",
      "199.92301201820374\n",
      "200.5528559088707\n",
      "201.04662209749222\n",
      "201.63173389434814\n",
      "202.5184274315834\n",
      "203.81279951334\n",
      "204.37249839305878\n",
      "205.16006708145142\n",
      "207.03444123268127\n",
      "208.59585642814636\n",
      "209.1234918832779\n",
      "209.64834189414978\n",
      "212.95808100700378\n",
      "213.5542825460434\n",
      "214.0747453570366\n",
      "214.5161376297474\n",
      "215.10259875655174\n",
      "218.0215798318386\n",
      "218.4589075744152\n",
      "219.25080302357674\n",
      "219.88391026854515\n",
      "223.13265308737755\n",
      "223.71794864535332\n",
      "224.48888537287712\n",
      "228.10706731677055\n",
      "228.75812515616417\n",
      "231.5182178914547\n",
      "232.00438770651817\n",
      "232.38306486606598\n",
      "236.1545354127884\n",
      "236.9815275669098\n",
      "Epoch 1/2 ,Iteration: 100 , Avg. Training loss: 2.3698 , Processing : 0.2595 sec/batch\n",
      "0.38521820306777954\n",
      "1.1650735139846802\n",
      "1.87041175365448\n",
      "5.688528895378113\n",
      "6.49129045009613\n",
      "7.951557874679565\n",
      "8.460854649543762\n",
      "9.182714819908142\n",
      "13.36340081691742\n",
      "13.91493046283722\n",
      "14.5109783411026\n",
      "16.95315945148468\n",
      "20.10222852230072\n",
      "21.978710651397705\n",
      "22.749255120754242\n",
      "26.581266582012177\n",
      "27.16904890537262\n",
      "27.769051134586334\n",
      "30.20992261171341\n",
      "30.924053728580475\n",
      "31.55512899160385\n",
      "33.817436158657074\n",
      "34.444733917713165\n",
      "35.6862775683403\n",
      "39.76310557126999\n",
      "40.227984100580215\n",
      "40.897562235593796\n",
      "42.110816806554794\n",
      "42.9249224960804\n",
      "45.1133818924427\n",
      "45.83056077361107\n",
      "46.63638690114021\n",
      "47.28466114401817\n",
      "49.03167042136192\n",
      "49.58687075972557\n",
      "50.28236082196236\n",
      "50.932282119989395\n",
      "51.5830235183239\n",
      "52.68414744734764\n",
      "53.67557027935982\n",
      "58.33773449063301\n",
      "59.03059282898903\n",
      "59.5838368833065\n",
      "60.16348943114281\n",
      "65.62459525465965\n",
      "70.64817580580711\n",
      "72.04000076651573\n",
      "77.3275097310543\n",
      "77.59201902151108\n",
      "81.19218081235886\n",
      "81.55447354912758\n",
      "84.62941566109657\n",
      "85.23080477118492\n",
      "89.03713092207909\n",
      "89.86886122822762\n",
      "90.73781964182854\n",
      "94.71768566966057\n",
      "96.13572439551353\n",
      "101.20071157813072\n",
      "106.30040058493614\n",
      "107.35037884116173\n",
      "111.85734352469444\n",
      "116.37181553244591\n",
      "118.32521173357964\n",
      "118.9378055036068\n",
      "121.48809704184532\n",
      "127.56940349936485\n",
      "132.21122679114342\n",
      "133.6816379725933\n",
      "139.43650552630424\n",
      "140.22005453705788\n",
      "141.5202946960926\n",
      "142.24057254195213\n",
      "143.2773971259594\n",
      "144.37534806132317\n",
      "145.24301847815514\n",
      "150.92238077521324\n",
      "151.8015715777874\n",
      "158.46656247973442\n",
      "160.51877996325493\n",
      "162.37967655062675\n",
      "167.65557500720024\n",
      "175.66737005114555\n",
      "176.93107470870018\n",
      "178.94015821814537\n",
      "179.68500700592995\n",
      "180.62888339161873\n",
      "181.877955108881\n",
      "182.81349369883537\n",
      "183.93136402964592\n",
      "189.67575642466545\n",
      "190.6646205484867\n",
      "192.6117333471775\n",
      "193.06458681821823\n",
      "193.91604447364807\n",
      "194.75854551792145\n",
      "195.8851261138916\n",
      "196.72250199317932\n",
      "200.21999263763428\n",
      "206.31261825561523\n",
      "Epoch 1/2 ,Iteration: 200 , Avg. Training loss: 2.0631 , Processing : 0.3000 sec/batch\n",
      "0.8613524436950684\n",
      "3.4998481273651123\n",
      "4.14271879196167\n",
      "4.784881830215454\n",
      "11.428399324417114\n",
      "16.233426332473755\n",
      "16.901235699653625\n",
      "18.91928732395172\n",
      "19.664006054401398\n",
      "21.450020611286163\n",
      "23.39424628019333\n",
      "31.495687663555145\n",
      "32.167808175086975\n",
      "34.30425727367401\n",
      "36.94096791744232\n",
      "37.63746243715286\n",
      "41.75654870271683\n",
      "44.38702565431595\n",
      "49.893692314624786\n",
      "50.81505066156387\n",
      "51.41679483652115\n",
      "51.92446714639664\n",
      "52.51986122131348\n",
      "58.884437084198\n",
      "59.63047605752945\n",
      "65.63664489984512\n",
      "66.38364386558533\n",
      "66.96210360527039\n",
      "74.34535336494446\n",
      "75.13321161270142\n",
      "82.0807695388794\n",
      "83.08805060386658\n",
      "87.73252701759338\n",
      "95.03921484947205\n",
      "102.77591252326965\n",
      "107.19062685966492\n",
      "110.89862132072449\n",
      "111.42470020055771\n",
      "112.45697337388992\n",
      "120.60308486223221\n",
      "121.07665902376175\n",
      "122.16634911298752\n",
      "130.0220324397087\n",
      "136.8224942088127\n",
      "144.05669230222702\n",
      "146.72791570425034\n",
      "147.6533562541008\n",
      "148.17865371704102\n",
      "148.69731509685516\n",
      "149.12588348984718\n",
      "149.73058959841728\n",
      "155.68005058169365\n",
      "156.35044702887535\n",
      "157.32880052924156\n",
      "157.72037687897682\n",
      "159.08294704556465\n",
      "169.06346347928047\n",
      "169.84074476361275\n",
      "170.29162514209747\n",
      "170.6872546672821\n",
      "178.7794349193573\n",
      "179.43345975875854\n",
      "180.39640468358994\n",
      "180.8841627240181\n",
      "187.68435841798782\n",
      "188.79405492544174\n",
      "189.45544493198395\n",
      "190.08091336488724\n",
      "191.710695207119\n",
      "194.30045002698898\n",
      "203.79276722669601\n",
      "208.72861450910568\n",
      "217.75827759504318\n",
      "219.60340827703476\n",
      "226.90230411291122\n",
      "227.4690603017807\n",
      "233.55337607860565\n",
      "238.19954907894135\n",
      "247.01292169094086\n",
      "249.07930719852448\n",
      "258.4898067712784\n",
      "267.6042333841324\n",
      "276.58344423770905\n",
      "284.503453373909\n",
      "285.6573739051819\n",
      "286.25541162490845\n",
      "288.35423517227173\n",
      "298.0226197242737\n",
      "305.620644569397\n",
      "307.27994203567505\n",
      "308.55185210704803\n",
      "317.9509505033493\n",
      "318.6011199951172\n",
      "321.02053213119507\n",
      "328.3625087738037\n",
      "329.2022887468338\n",
      "331.7470737695694\n",
      "342.6822029352188\n",
      "349.43574488162994\n",
      "350.15361285209656\n",
      "Epoch 1/2 ,Iteration: 300 , Avg. Training loss: 3.5015 , Processing : 0.3123 sec/batch\n",
      "2.107809543609619\n",
      "3.7839667797088623\n",
      "7.703233003616333\n",
      "8.4831303358078\n",
      "9.085586488246918\n",
      "10.73308140039444\n",
      "11.401129961013794\n",
      "21.579447984695435\n",
      "33.28323578834534\n",
      "34.30840790271759\n",
      "35.26374417543411\n",
      "36.44996351003647\n",
      "44.29764264822006\n",
      "44.97638475894928\n",
      "50.20647442340851\n",
      "62.13997948169708\n",
      "74.38226330280304\n",
      "79.85599339008331\n",
      "92.47549164295197\n",
      "93.10270190238953\n",
      "94.13667154312134\n",
      "95.30689871311188\n",
      "96.00761246681213\n",
      "109.04701542854309\n",
      "109.78941637277603\n",
      "113.12853556871414\n",
      "114.08722281455994\n",
      "126.12680983543396\n",
      "127.11488652229309\n",
      "140.44051480293274\n",
      "148.28664803504944\n",
      "149.0651074051857\n",
      "149.8116289973259\n",
      "154.29443806409836\n",
      "154.80913561582565\n",
      "167.34222143888474\n",
      "174.0362450480461\n",
      "183.79018181562424\n",
      "191.27232998609543\n",
      "194.95269268751144\n",
      "195.6499188542366\n",
      "198.3992674946785\n",
      "198.97563880681992\n",
      "200.0003804564476\n",
      "211.0383214354515\n",
      "219.54847902059555\n",
      "220.08246475458145\n",
      "230.81004482507706\n",
      "231.33701223134995\n",
      "238.0202574133873\n",
      "238.6526330113411\n",
      "240.75428587198257\n",
      "242.28323537111282\n",
      "249.99682265520096\n",
      "251.71632343530655\n",
      "264.04660564661026\n",
      "277.03033500909805\n",
      "282.85938984155655\n",
      "293.76916939020157\n",
      "299.05544525384903\n",
      "310.904304087162\n",
      "316.8153663277626\n",
      "329.0241399407387\n",
      "343.0860839486122\n",
      "348.3175340294838\n",
      "349.2904260754585\n",
      "355.64016515016556\n",
      "359.9235857129097\n",
      "360.6600778102875\n",
      "367.83248019218445\n",
      "368.5358603000641\n",
      "369.3186534643173\n",
      "381.9614986181259\n",
      "388.1984747648239\n",
      "390.5734750032425\n",
      "403.14002430438995\n",
      "410.0208560228348\n",
      "412.3456791639328\n",
      "420.6696318387985\n",
      "433.04110419750214\n",
      "445.10608756542206\n",
      "451.43894708156586\n",
      "452.0941768884659\n",
      "465.0500017404556\n",
      "474.8338667154312\n",
      "475.4461140036583\n",
      "476.1361837387085\n",
      "477.84825456142426\n",
      "479.5248569250107\n",
      "483.0506421327591\n",
      "483.8381029367447\n",
      "485.61037480831146\n",
      "499.44925248622894\n",
      "503.72694861888885\n",
      "504.4283395409584\n",
      "518.0420413613319\n",
      "521.4712360501289\n",
      "522.1990422010422\n",
      "524.8130921125412\n",
      "531.3308609724045\n",
      "Epoch 1/2 ,Iteration: 400 , Avg. Training loss: 5.3133 , Processing : 0.3170 sec/batch\n",
      "0.9490600824356079\n",
      "3.2922006845474243\n",
      "16.413118958473206\n",
      "17.15597152709961\n",
      "30.8638277053833\n",
      "43.90872669219971\n",
      "46.657896518707275\n",
      "59.48031759262085\n",
      "60.284389555454254\n",
      "72.9718227982521\n",
      "85.25165563821793\n",
      "87.09684592485428\n",
      "95.0793064236641\n",
      "108.40543347597122\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-23ee8fd34a41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Creating the model checkpoint directory\"\"\"\n",
    "# !mkdir model_checkpoint # if the dir do not exist, run the code\n",
    "epochs = 2 # Increase it as per computation resources. It has been kept low here for users to replicate the process, increase to 100 or more.\n",
    "batch_length = 1000\n",
    "word_window = 10\n",
    "\n",
    "with tf_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=tf_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        batches = skipG_batch_creation(train_words, batch_length, word_window)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            train_loss, _ = sess.run([cost_fn, optim], feed_dict={input_:x, label_:np.array(y)[:, None]})\n",
    "            \n",
    "            loss += train_loss\n",
    "            print(loss)\n",
    "            if iteration % 100 == 0:\n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs), \",Iteration: {}\".format(iteration), \n",
    "                      \", Avg. Training loss: {:.4f}\".format(loss/100), \", Processing : {:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "                \n",
    "            if iteration % 2000 == 0:\n",
    "                similarity_ = word_similarity.eval()\n",
    "                for i in range(validation_cnt):\n",
    "                    validated_words = rev_dictionary_[validation_words[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-similarity_[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s: ' % validated_words\n",
    "                    for k in range(top_k):\n",
    "                        close_word = rev_dictionary_[nearest[k]]\n",
    "                        log = '%s %s, ' % (log, close_word)\n",
    "                    print(log)\n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"model_checkpoint/skipGram_test8.ckpt\")\n",
    "    embed_mat = sess.run(normalization_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 所有其他迭代也将打印出类似的输出结果，经过训练的网络将被还原，供以后使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The Saver class adds ops to save and restore variables to and from checkpoints.\"\"\"\n",
    "with tf_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session(graph = tf_graph) as sess:\n",
    "    \"\"\"Restaring the trained network\"\"\"\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('model_checkpoint'))\n",
    "    embed_mat = sess.run(word_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用t分布随机邻嵌入(t-SNE)来实现可视化\n",
    "### 250个随机单词的300度高维向量表示已经在二维向量空间中使用\n",
    "### t-SNE确保了向量的初始结构可以在新维度中被保留，甚至是在转换后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TSNE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-90f87a13c7e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mword_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mword_embedding_tsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mword_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TSNE' is not defined"
     ]
    }
   ],
   "source": [
    "word_graph = 250\n",
    "tsne = TSNE()\n",
    "word_embedding_tsne = tsne.fit_transform(embed_mat[:word_graph, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 具有语义相似性的单词在其二维空间中表示中彼此更接近，从而及时在维度进一步减小之后也保持着它们的相似性。\n",
    "### 注入year、years和age之类的单词的位置较为接近，并且与international和religious等单词距离较远。\n",
    "### 训练模型是可以采用更多迭代，一实现更好的词嵌入表示，并且通过改变阈值来调整结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
