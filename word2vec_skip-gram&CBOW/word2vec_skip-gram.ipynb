{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec_skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing the required packages'''\n",
    "import random\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "'''Make sure the dataset link is copied correctly'''\n",
    "dataset_link = 'http://mattmahoney.net/dc/'\n",
    "zip_file = 'text8.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载有Matt Mahoney 收集和清理的危机百科文章数据集，并将其存储为当前工作目录下的单独文件\n",
    "def data_download(zip_file):\n",
    "    \"\"\"Download the required file\"\"\"\n",
    "    if not os.path.exists(zip_file):\n",
    "        zip_file, _ = urlretrieve(dataset_link + zip_file, zip_file)\n",
    "        print('File downloaded successfully!')\n",
    "    return None\n",
    "data_download(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩的文本数据集在内部文件夹数据集中提取，稍后将用于训练模型\n",
    "\"\"\"Extracting the dataset in separate folder\"\"\"\n",
    "extracted_folder = 'dataset'\n",
    "\n",
    "if not os.path.isdir(extracted_folder):\n",
    "    with zipfile.ZipFile(zip_file) as zf:\n",
    "        zf.extractall(extracted_folder)\n",
    "        \n",
    "with open('dataset/text8') as ft_:\n",
    "    full_text = ft_.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于输入数据的文本中有多个标点符号和其他符号，相同的符号将被替换为带有标点符号名称和符号类型的相应字符\n",
    "# 有助于让模型单独识别每个标点符号和其他符号并生成向量\n",
    "def text_processing(ft8_text):\n",
    "    \"\"\"Replacing punctuation marks with tokens\"\"\"\n",
    "    ft8_text = ft8_text.lower()\n",
    "    ft8_text = ft8_text.replace('.', '<period>')\n",
    "    ft8_text = ft8_text.replace(',', '<comma>')\n",
    "    ft8_text = ft8_text.replace('\"', '<quotation>')\n",
    "    ft8_text = ft8_text.replace(';', '<semicolon>')\n",
    "    ft8_text = ft8_text.replace('!', '<exclamation>')\n",
    "    ft8_text = ft8_text.replace('?', '<question>')\n",
    "    ft8_text = ft8_text.replace('(', '<paren_l>')\n",
    "    ft8_text = ft8_text.replace(')', '<paren_r>')\n",
    "    ft8_text = ft8_text.replace('--', '<hyphen>')\n",
    "    ft8_text = ft8_text.replace(':', '<colon>')\n",
    "    ft8_text_tokens = ft8_text.split()\n",
    "    return ft8_text_tokens\n",
    "\n",
    "ft_tokens = text_processing(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including']\n"
     ]
    }
   ],
   "source": [
    "# 为了提高所产生的向量表示的质量，建议去除与单词相关的噪音，即输入数据集中词频小于7的单词，因为这些单词没有足够的信息来提供它们的上下文\n",
    "# 可以通过检查单词数和数据集中的分布来调整此阈值，在此处设为7\n",
    "\"\"\"Shortlisting words with frequency more than 7\"\"\"\n",
    "word_cnt = collections.Counter(ft_tokens)\n",
    "shortlisted_words = [w for w in ft_tokens if word_cnt[w] > 7]\n",
    "\n",
    "# 列出数据集中词频最高的几个单词\n",
    "print(shortlisted_words[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of shortlisted words :  16616688\n",
      "Unique number of shortlisted_words:  53721\n"
     ]
    }
   ],
   "source": [
    "# 检查数据集中所有单词的统计信息\n",
    "print(\"Total number of shortlisted words : \", len(shortlisted_words))\n",
    "print(\"Unique number of shortlisted_words: \", len(set(shortlisted_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了处理语料库中存在的独特单词，我们制作了一组单词和它们在训练数据集中的词频\n",
    "# 创建一个字典并将单词转换为整数，反之，将整数转换为单词\n",
    "# 词频最高的单词被赋予最小值0， 其他单词也通过相似方式被赋予数值，从单词转换而来的整数倍存储在一个单独的数组中\n",
    "def dict_creation(shortlisted_words):\n",
    "    \"\"\"The function creates a dictionary of the words present in dataset along with their frequency order\"\"\"\n",
    "    counts = collections.Counter(shortlisted_words)\n",
    "    vocabulary = sorted(counts, key=counts.get, reverse=True)\n",
    "    rev_dictionary_ = {ii: word for ii, word in enumerate(vocabulary)}\n",
    "    # print(rev_dictionary_)\n",
    "    dictionary_ = {word: ii for ii, word in rev_dictionary_.items()}\n",
    "    # print(dictionary_)\n",
    "    return dictionary_, rev_dictionary_\n",
    "\n",
    "dictionary_, rev_dictionary_ = dict_creation(shortlisted_words)\n",
    "words_cnt = [dictionary_[word] for word in shortlisted_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram 模型采用子采样的方法来处理文本中的停止词\n",
    "### 通过在词频上设置阈值，可以消除所有那些词频较高且中心词周围没有任何重要上下文的单词，这带来了更快的训练速度和更好的词向量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram论文中给出的概率分数函数，对于训练集中的每个单词，我们将根据以下公式给定的概率来决定是否将其移除\n",
    "$$ P(w_{i}) = 1-\\left( \\sqrt\\frac{t}{f(w_{i})}\\right)$$\n",
    "### 其中, t是阈值参数，$f(w_{i})$是单词$w_i$在总数据集中的词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "5233",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-8d34099ac669>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfreqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_count\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mp_drop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_cnt\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mp_drop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-8d34099ac669>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfreqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_count\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mp_drop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_cnt\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mp_drop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 5233"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\"\"\"Creaing the threshold and performing the subsampling\"\"\"\n",
    "thresh = 0.00005\n",
    "word_counts = collections.Counter(word_cnt)\n",
    "total_count = len(words_cnt)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1-np.sqrt(thresh/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in words_cnt if p_drop[word]<random.random()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
